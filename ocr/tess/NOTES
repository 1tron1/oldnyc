General approach:
x Write a script to crop an image based on tesseract boxes
  -> Assess accuracy on 10 images
    710374b perfect
    715705b minor problem: missing first line ("(1)")
    718035b major problem: missing first line, last 4 and rightmost ~20 cols
            Box detection fails spectacularly on this one! Contrast is low.
            A better crop fixes this --> re-use my image cropper?
            Running unpaper with no arguments helps, too.
    721046b perfect (dark)
    722975b perfect
    726132b perfect
    726232b perfect
    726358b perfect (rotated)
    727425b perfect
    732437b perfect

  Running unpaper as a pre-processor (before makebox) fixes the major problem
  but not the minor problem --> probably a win?

  This sequence generates accurate crops:
    for x in $(ls *.reconstructed.jpg); do convert $x /tmp/blah.pgm; unpaper --overwrite /tmp/blah.pgm /tmp/unpaper.pgm; convert /tmp/unpaper.pgm ${x/reconstructed/unpapered}.png; done
    for x in $(ls *.unpapered.png); tesseract $x $(basename $x .png) batch.nochop makebox
    for x in $(ls *.unpapered.box); ~/github/oldnyc/ocr/tess/crop_to_box.py $x ${x/unpapered.box/reconstructed.jpg} ${x/unpapered.box/cropped.png}

x Write a script to generate histograms of box widths

    See https://docs.google.com/spreadsheets/d/1X7myP815ruxSD-UJ1CTp2BoP95svQJCvaEl-Wy2yO7Q/edit#gid=0

    Clear peaks @ 12, 24, 36
    Almost nothing @ w=16, 17, 18, 19

    16: (bad, handwritten) 727425b.cropped.box ” 138 101 154 105 0
    16: (legit; oddly wide) 732437b.cropped.box m 292 26 308 38 0
    17: (bad, handwritten) 727425b.cropped.box * 257 169 274 180 0
    17: (bad, handwritten) 727425b.cropped.box 3 317 157 334 177 0
    17: (bad, handwritten) 727425b.cropped.box 5 335 156 352 177 0
    18: (bad, handwritten) 727425b.cropped.box Z 113 90 131 113 0
    19: (bad, handwritten) 718035b.cropped.box m 525 107 544 119 0
    19: (bad, handwritten) 721046b.cropped.box 7 225 21 244 52 0

    --> 20+, split it!

x Write a script to split boxes & add artificial white lines

- Compare accuracy of boxes on:
  - Cropped + binarized images (using Otsu or local adaptive)
  - Cropped + split images


-----

  The "unpaper" step helped with my initial sample of 10, but in a subsequent
pass on a sample of 25 there was another over-cropped image, even after
unpapering. I'm thinking that an image morphology approach (similar to what I
did for detecting photos on cards) might be better.

    brew install homebrew/python/scipy

Of my 25 sample images,

    _ have no border

    1 has a small border and brown paper
    6 have a small border and gray paper

    -> 7 have black border + text on gray paper
    -> 18 have black border, gray border, text on white paper

    1 has a watermark

    -> In all cases, cropping to the bbox of the largest black component will
       get rid of the first border.

    The text is typically centered. I could come up with some kind of score
    function for each row that gives greater weight to:
      1. white pixels near the center
      2. runs of white pixels

    Maybe each pixel is worth:
      e^-|distance from center| * (length of run)

    ... this doesn't distinguish short, off-center lines from the background.

Another approach is to:
1. Remove borders
2. Run canny edge detector
3. Select areas with lots of variance
  
  
Want to find the smallest rectangle that accounts for the most content.
-> f1 score!

My strategy was to start with the full image rectangle, then keep removing a
row or column from the left/right/top/bottom, whichever had the fewest white
pixels. This is greedy, which prevents it from working.
  -> maybe crop x & y independently?

Or do dilation to come up with larger components.

----

Sample of 50:
  - With four groups of text, missing the bottom-most (this is the worst issue)
  - Missing attribution block at the bottom (be more aggressive about recall?)
  - Missing "(1)", "(2)" on left side
  - Missing "(1)", "(2)" on left side
  - Missing "(1)", "(2)" on left side
  - Missing "(1)" on top, includes stamp outside border
  - Missing "(1)", "(2)" on left side
  - Includes "NYPL" stamp (solution: remove areas outside any borders)
  - Includes a smudge outside the border
  - Inluding a stamp outside the border
  - Inluding a stamp outside the border
  (other 39 were perfect)

Another sample of 25:
  - 724844b -- missing "(1)", "(2)" on left side
  - 721235b -- missing "(1)" on top
  - 722563b -- including a stamp on the bottom
  - 721028b -- missing a "(1)" on the top

(added a special condition for adding high-impact rectangles to get the last few bits)

Another sample of 25:
  + 713992b -- grab a wrinkle on the left side of the paper
  - 724294b -- entire sheet
  + 714291b -- includes stamp outside border
  - 724548b -- entire sheet
  - 710461b -- entire paper

My border detection technique is also affecting the text.
Maybe detecting & removing borders explicitly is a better idea.

Another sample of 25, it was basically perfect.
Another sample of 25,

  1 overcropped
  2 undercropped (708848b, 729772b)

-- Tweaks: fix a floating point vs. int bug (new_area_frac = 0 always!)
           start with the largest component, allow a component to be dropped
           even if it has >10% of the pixels.
    .. pretty much perfect!

    718518b: overcrop!
    730339b: slight overcrop
    708376b: slight overcrop, dropped date

    721028b: cut off "(1)"
    730616b: cut off "(1)"
    724264b: cut off "(1)", "(2)", "(3)"
    705413b: cut off "(1)"

    --> 97% accuracy

---

Now that cropping is working well, I'm sad to see that it doesn't improve OCR
accuracy that much. I need to come up with a way of quantifying this, but I was
hoping for a clear win.

I transcribed 25 images manually.
First stab at a score was Levenshtein(golden, actual) / len(golden). This
produces a score in [0, 1], but it's a very easy curve. Even transcriptions
which don't look very great score above 90%.

The average of raw tesseract on these images was 0.772.

Ideas to test:

- Crop

    Brings the average up from 0.772 --> 0.863; Scores above 0.9 from 7-->14.
    All crops were perfect.

- Crop & apply box splitting

    Average: 0.855, i.e. worse than with cropping alone
    --> if this is going to work, it needs to be more targeted.

    Splitting the wide letters is having side effects: it results in letters
    being vertically joined, which is far worse! I asked about this on the
    tesseract-ocr list.

- Crop & apply adaptive filter
    --> doesn't work very well! The image is mostly background.

- Apply box splitting w/o cropping

- Use a higher-resolution image

- Get consensus forms for each letter and convolve the image with each.


---------

New ideas for approaches forward:

- Extract images for each character (using modified box files) and run them
  through Tesseract with psm=10 (single character mode).

  --> This won't work without additional Tesseract training.
      Of four sample "b" images, it only gets two correct.

- Extract images for each character (using modified box files) and look for the
  closest match amongst a large set of hand-extracted characters from other
  images.

    --> cv2.matchTemplate works well. I can match just the three 'a's in a line
        with a threshold of r=0.9.
        A strict threshold might not even be necessary—I could use the max
        correlation of any letter image within a box.

- Use something other than Tesseract
  --> ocropy has really nice segmentation tools
  --> works significantly better on the higher-resolution imagery

  A way forward with ocropy would be to use its segmentation/page layout tools
  to get down to individual lines, then do pattern matching on each of these.
  Binarization may be too much of a reduction for the 15px letters.
  Four- or eight-level grayscale might work better for pattern matching.

  I initially had some trouble training ocropy because I fed it whole images
  plus ground truth for the whole images. In reality, it wants binarized lines
  plus ground truth for those binarized lines. It was only in watching the
  YouTube video about training ocropy that I realized this.

  I ran ocropus-rtrain for about 48 hours before killing it. Results started to
  get worse around 50,000 or 60,000 iterations. I suspect overall results will
  be decent. One thing I find baffling about ocropy is that there's no simple
  way to get plain text out of it!

  I believe ocropy would do significantly better with higher resolution imagery.

  A downside of ocropy is that it's quite slow.

  --> FineReader Pro is amazing.
      Its average out of the box is 0.86, but that's a serious under-estimate.
      It's being penalized for OCRing hand-written text which I didn't put in
      the truth data. It has 12/25 docs at >0.95.

      This is a bit demoralizing. Open source OCR is way behind.


ocropus
-------
Using training/typewriterb-00050000.pyrnn.gz as model
Running over 25 images (which were used in training).

  evaluate_ocr.py average is 0.818
  Most errors are due to layout issues, i.e. ordering of the output lines.
  The text transcription itself is fantastic.

  Could I do better myself? Using a variant of ordering from top to bottom
  might work well.
  --> This gets the average up to 0.932, the best score I've seen yet.
      18/25 have scores >0.9 and 16/25 have scores >0.95.

  Things to try:
  - Using my cropper (would remove noisy extracted lines & improve speed)
  - Set `-n` on `ocropus-rpred` to pick up "(1)"-style lines.
  - Train with a smaller character set (only letters in the truth data).
      (this might not matter—it's unlikely to find a letter it's never seen)
  - Run evaluate_ocr.py on the .ocr.txt files that David provided.
      --> It scores 0.729. 5/24 >0.95, 10/24 > 0.9.
          Anecdotally, ocropus does much better, though there are counterexamples.
  - Create a setup for blind comparisons: which does better?
  - Train a model starting with the pre-built one
  - Run all ~70 models (from every 1000 iterations) on test set to figure out
    when it maxes out.

Timing:
  nlbin:    8.0s/image
  gpageseg: 4.7s/image
  rpred:    1.6s/image (model loading amortized)
  Total in practice is ~15s/image

There are a decent number of images which are rotated 90 or 180 degrees (or
flipped).

  On a pure test set (no inputs it's seen before), ocropy gets a score of
  0.860. This includes a 0.0 for an image with no type-written text. Excluding
  that, it gets a 0.90 average. 11/24 are >0.95 and 14/24 are >0.9.

#### Cropping

crop_morphology.py takes ~1s/image.
It reduces the runtime for nlbin to ~1.5s/image (from 8s/image).
gpageseg is also faster: ~1s/image (from 4.7s/image).

The images of lines are notably lighter, but also clearer to my eyes.

Using the typewriterb-00048000 model (which was trained on the darker images),
it gets an average of 0.908, with 19/25 >0.95. There are two particularly bad
ones (0.284 and 0.489). There's a new type of error on 734090b: two lines get
merged together, wreaking havoc on the character recognition. Training wouldn't
fix this issue. (702261b is another example)

Setting a max height would help. This happens for exactly 1/238 images.
Setting --scale 11 --minscale 11 does the trick.
But will that have other negative consequences?
  --> for ocropy-25, there was only one problematic split w/ scale=11.
  --> for 2-ocropy-25, there were no problematic merges w/o explicit scale.
      there were a few bad splits with scale=11. :(
  --> not setting scale is the lesser of two evils. If I deal with this issue,
      it's going to be by modifying gpageseg.

Overall, cropping seems like a win!


#### The pre-built ocropus model.

The en-default.pyrnn.gz performs quite poorly out of the box.
This is likely because it's never seen ALLCAPS data, or typewriter fonts, as
they say on the ocropus web site.

Presumably it knows something about English and letter forms, though, so it
might be worth training on top of it, rather than from scratch.

For reference, transcribing 437 lines by hand took just north of an hour using
Preview and vim. Using localturk, another 417 took about 40 minutes.

- Training off of the pre-built model works amazingly well!!!
  With 2000 generations of training on top of it using the ~800 hand-labeled,
  cropped lines, I got a score of 0.936 on the test set.
  Excluding the 0.0 image, that's a ridiculous score of 0.975. 20/25 are >0.95.

After ~15,000 iterations, the model is insanely good.
As I look through the errors, I'm finding more typos in the ground truth than
genuine mistakes, e.g. the one image where it actually says "NO REPRIDUCTIONS."
  722265b-crop-010001.png is a great example: it has a typo: "ia" instead of "is".

  Fixing these errors in the GT dropped the error rate on the 14k model from
  0.57% to 0.39%.

After 21,000 iterations, average score is 0.942, 0.981 excluding the
hand-written card. Most of the mistakes are single-character errors which might
be catchable with a spell checker.

#### What is the "ALN" line in training?

It seems to be much more accurate than the "OUT" line, and it improves much
more quickly at the beginning.

When you run `ocropus-rpred`, you get the OUT line, not the ALN line.

tomdev says that it incorporates the truth data, so it's cheating.


#### Learning history

See http://goo.gl/t5kUoi for a spreadsheet.
I trained ocropus using ~400 input images and tested its accuracy on both those
images and another set of ~400 test images. The performance on the two tracked
each other very well, with the test error rate consistently about 3% higher
than the train error rate.

- The model seems to diverge—the chart is essential for picking a good stop.
- Occasionally the model gets much worse. Is this an intentional shake-up, to
  get out of a local minimum? Or something else?


#### Page Orientation

Of the 809 images David sent me, seven are landscape views.
  - 3 are rotated 90 degrees clockwise
  - 4 are legitimately landscape (though they don't have any typewritten text)

Some are probably upside-down, too.
  --> maybe not? I looked at 200 of the images and didn't see a single one.
      724096b is upside-down

So: rotate landscape images counterclockwise 90 degrees. Easy!

  705207b is an example of over-cropping.

#### Tesseract/Ocropus comparison

I did a blind trial where I chose between raw Tesseract & Ocropus output for
each of 20 images. For all 20, I either chose Ocropus or said that they were
about the same. In many cases the difference was dramatic. Ocropus had a few
typos, but the only really bad problem was with an upside-down image.

